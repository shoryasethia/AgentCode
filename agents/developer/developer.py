# developer/developer.py

from typing import Dict, Any, List
from langchain_core.messages import AIMessage, SystemMessage, HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, START, END
from state import DeveloperState, AtomicTask, TaskType
import shutil
from pathlib import Path


class DeveloperAgent:
    def __init__(self, llm: ChatGoogleGenerativeAI):
        self.llm = llm
    
    def initialize_development(self, state: DeveloperState) -> Dict[str, Any]:
        """Initialize development phase"""
        atomic_tasks = state.get("atomic_tasks", [])
        if not atomic_tasks:
            return {**state, "current_phase": "complete"}
        
        return {
            **state,
            "current_task_index": 0,
            "current_task": atomic_tasks[0],
            "current_phase": "implementation", 
            "task_completion_status": {},
            "retry_count": 0
        }
    
    def execute_task_implementation(self, state: DeveloperState) -> DeveloperState:
        """Execute the implementation for current task"""
        current_task = state.get("current_task")
        if not current_task:
            return state
        
        workspace_path = Path(state.get("workspace_path", "."))
        files_modified = list(state.get("files_modified", []))
        files_created = list(state.get("files_created", []))
        files_deleted = list(state.get("files_deleted", []))
        errors = list(state.get("errors_encountered", []))
        success = False
        
        try:
            task_type = current_task.get("type")
            
            if task_type == TaskType.CREATE_FILE:
                success = self._create_file(current_task, workspace_path, files_created, errors)
            elif task_type == TaskType.MODIFY_FILE:
                success = self._modify_file(current_task, workspace_path, files_modified, errors)
            else:
                success = self._modify_file(current_task, workspace_path, files_modified, errors)

            task_completion_status = state.get("task_completion_status", {})
            task_completion_status[current_task["id"]] = success
            
            return {
                **state,
                "files_modified": files_modified,
                "files_created": files_created,
                "files_deleted": files_deleted,
                "task_completion_status": task_completion_status,
                "errors_encountered": errors,
                "current_phase": "validation"
            }
            
        except Exception as e:
            errors.append(str(e))
            return {
                **state,
                "errors_encountered": errors,
                "current_phase": "validation"
            }
    
    def _create_file(self, task: AtomicTask, workspace_path: Path, files_created: List, errors: List) -> bool:
        """Create a new file with content generated by the LLM."""
        try:
            target_files = task.get('target_files', [])
            if not target_files:
                errors.append(f"Cannot create file: No target_files specified for task {task.get('id')}")
                return False

            system_prompt = "You are an expert programmer. Your task is to write the full content for a new file. Return ONLY the raw code or text for the file. Do not include any explanations, comments, or markdown formatting like ```python or ```cpp."
            human_prompt = f"The file should be created based on this description: \"{task.get('description')}\""
            
            for target_file in target_files:
                file_path = workspace_path / target_file
                file_path.parent.mkdir(parents=True, exist_ok=True)
                
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=human_prompt)
                ]
                response = self.llm.invoke(messages)
                response_content = response.content if isinstance(response.content, str) else str(response.content)
                
                # Clean up potential markdown formatting just in case
                if response_content.strip().startswith("```") and response_content.strip().endswith("```"):
                    response_content = "\n".join(response_content.strip().split('\n')[1:-1])

                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(response_content)
                
                files_created.append(str(file_path))
            
            return True
        except Exception as e:
            errors.append(f"Error creating file: {e}")
            return False
    
    def _modify_file(self, task: AtomicTask, workspace_path: Path, files_modified: List, errors: List) -> bool:
        """Modify an existing file based on the task description."""
        try:
            target_files = task.get('target_files', [])
            if not target_files:
                 errors.append(f"Cannot modify file: No target_files specified for task {task.get('id')}")
                 return False

            for target_file in target_files:
                file_path = workspace_path / target_file
                if not file_path.exists():
                    errors.append(f"Cannot modify file: {file_path} does not exist.")
                    continue
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    current_content = f.read()
                
                system_prompt = "You are an expert programmer. Your task is to modify a file. Return the COMPLETE, modified file content. Do NOT add explanations or markdown wrappers."
                human_prompt = f"Modify the file '{target_file}' to accomplish the following task: \"{task.get('description')}\"\n\nHere is the current content of the file:\n```\n{current_content}\n```"
                
                messages = [
                    SystemMessage(content=system_prompt),
                    HumanMessage(content=human_prompt)
                ]
                response = self.llm.invoke(messages)
                response_content = response.content if isinstance(response.content, str) else str(response.content)

                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(response_content)
                
                files_modified.append(str(file_path))
            
            return True
        except Exception as e:
            errors.append(f"Error modifying file {task.get('target_files')}: {e}")
            return False

    def validate_task_completion(self, state: DeveloperState) -> DeveloperState:
        """Validate that the current task was completed successfully"""
        current_task = state.get("current_task")
        if not current_task:
            return state
        
        task_id = current_task["id"]
        was_successful = state.get("task_completion_status", {}).get(task_id, False)
        
        validation_message = AIMessage(content=f"Task {task_id} validation: {'SUCCESS' if was_successful else 'FAILED'}")
        
        current_validation_results = list(state.get("validation_results", []))
        current_validation_results.append(validation_message)
        
        return {
            **state,
            "validation_results": current_validation_results,
            "current_phase": "next_task" if was_successful else "retry"
        }
    
    def move_to_next_task(self, state: DeveloperState) -> Dict[str, Any]:
        """Move to the next atomic task"""
        current_index = state.get("current_task_index", 0)
        atomic_tasks = state.get("atomic_tasks", [])
        
        next_index = current_index + 1
        
        if next_index >= len(atomic_tasks):
            return { **state, "current_phase": "complete", "current_task": None }
        
        return {
            **state,
            "current_task_index": next_index,
            "current_task": atomic_tasks[next_index],
            "current_phase": "implementation", # Go straight to implementation for next task
            "retry_count": 0
        }
    
    def retry_current_task(self, state: DeveloperState) -> Dict[str, Any]:
        """Retry the current task"""
        retry_count = state.get("retry_count", 0)
        max_retries = state.get("max_retries", 3)
        
        if retry_count >= max_retries:
            return self.move_to_next_task(state)
        
        return {
            **state,
            "retry_count": retry_count + 1,
            "current_phase": "implementation", # Retry by going straight to implementation again
            "errors_encountered": []
        }
    
    def create_developer_graph(self):
        """Create a lean developer workflow graph to avoid rate limits."""
        workflow = StateGraph(DeveloperState)
        
        workflow.add_node("initialize", self.initialize_development)
        workflow.add_node("implement", self.execute_task_implementation)
        workflow.add_node("validate", self.validate_task_completion)
        workflow.add_node("next_task", self.move_to_next_task)
        workflow.add_node("retry_task", self.retry_current_task)
        
        workflow.add_edge(START, "initialize")
        workflow.add_conditional_edges(
            "initialize",
            lambda x: "implement" if x.get("current_task") else END,
            {"implement": "implement", END: END}
        )
        
        workflow.add_edge("implement", "validate")
        
        workflow.add_conditional_edges(
            "validate",
            lambda x: "next_task" if x.get("current_phase") == "next_task" else "retry_task",
            {"next_task": "next_task", "retry_task": "retry_task"}
        )
        
        workflow.add_conditional_edges(
            "next_task",
            lambda x: "implement" if x.get("current_phase") == "implementation" else END,
             {"implement": "implement", END: END}
        )

        workflow.add_conditional_edges(
            "retry_task",
            lambda x: "implement" if x.get("current_phase") == "implementation" else "next_task",
            {"implement": "implement", "next_task": "next_task"}
        )
        
        return workflow.compile()


def create_developer_service(llm: ChatGoogleGenerativeAI):
    """Factory function to create developer service"""
    developer = DeveloperAgent(llm)
    return developer.create_developer_graph()
